name: Helm Integration Tests

on:
  workflow_call:
    inputs:
      # needed to be set if repo name differs in quay
      QUAY_REPO:
        required: false
        type: string
      # set this for service PRs to select tests pertaining to the service under test
      # must match the marker used for the service, please look at the `markers` section of pyproject.toml
      SERVICE_TO_TEST:
        required: false
        type: string
      TEST_REPO_BRANCH:
        required: false
        type: string
        default: master
    secrets:
      CI_AWS_ACCESS_KEY_ID:
        required: true
      CI_AWS_SECRET_ACCESS_KEY:
        required: true
      QA_DASHBOARD_S3_PATH:
        required: true
      QA_HELM_ARTIFACT_S3_PATH:
        required: true
      CI_TEST_ORCID_USERID:
        required: true
      CI_TEST_ORCID_PASSWORD:
        required: true
      CI_TEST_RAS_USERID:
        required: true
      CI_TEST_RAS_PASSWORD:
        required: true
      CI_TEST_RAS_2_USERID:
        required: true
      CI_TEST_RAS_2_PASSWORD:
        required: true
      CI_SLACK_BOT_TOKEN:
        required: true
      CI_SLACK_CHANNEL_ID:
        required: true
      EKS_CLUSTER_NAME:
        required: true

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  integration_tests:
      runs-on: self-hosted

      defaults:
        run:
          # the test directory in gen3-code-vigil
          working-directory: gen3-integration-tests

      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        REPO: ${{ github.event.repository.name }}
        REPO_FN: ${{ github.event.repository.full_name }}
        BRANCH: ${{ github.event.pull_request.head.ref }}
        PR_NUM: ${{ github.event.pull_request.number }}
        COMMIT_SHA: ${{ github.event.pull_request.head.sha }}
        RUN_NUM: ${{ github.run_number }}
        CI_TEST_ORCID_USERID: ${{ secrets.CI_TEST_ORCID_USERID }}
        CI_TEST_ORCID_PASSWORD: ${{ secrets.CI_TEST_ORCID_PASSWORD }}
        CI_TEST_RAS_USERID: ${{ secrets.CI_TEST_RAS_USERID }}
        CI_TEST_RAS_PASSWORD: ${{ secrets.CI_TEST_RAS_PASSWORD }}
        CI_TEST_RAS_2_USERID: ${{ secrets.CI_TEST_RAS_2_USERID }}
        CI_TEST_RAS_2_PASSWORD: ${{ secrets.CI_TEST_RAS_2_PASSWORD }}
        SLACK_CHANNEL: ${{ secrets.CI_SLACK_CHANNEL_ID }}
        EKS_CLUSTER_NAME : ${{ secrets.EKS_CLUSTER_NAME }}

      steps:
        # Ensure the PR is run under the same org as an Internal PR
        # and not by external forks/PRs
        - name: Check if PR is from the same organization
          if: github.repository_owner != github.event.pull_request.head.repo.owner.login
          run:  |
            echo "Skip pull requests from repositories not within the same organization"
            echo "SKIP_TESTS=true" >> $GITHUB_ENV

        # Skip integration tests when the following PR labels are present:
        # not-ready-for-ci / decommission-environment
        - name: Skip integration tests for specific PR labels
          working-directory: ${{ github.workspace }}
          run: |
            if gh api repos/$REPO_FN/pulls/$PR_NUM --jq '.labels | map(.name) | .[] | select(. == "not-ready-for-ci" or . == "decommission-environment")' | grep -q .; then
                echo "Skipping CI since one of the PR labels is present - not-ready-for-ci / decommission-environment"
                echo "SKIP_TESTS=true" >> $GITHUB_ENV
            fi

        # Checkout current repo
        - name: Checkout current repo
          if: ${{ env.SKIP_TESTS != 'true' }}
          uses: actions/checkout@v4

        # Skip tests when there are only markdown files
        - name: Skip integration tests if PR contains only Markdown files
          if: ${{ env.SKIP_TESTS != 'true' }}
          working-directory: ${{ github.workspace }}
          run: |
            git fetch -q
            FILE_TYPES=$(git show --name-only ${{ env.COMMIT_SHA }} | grep -o '\S\+\.\S\+'  | grep -v '@' | awk -F . '{print $NF}' | sort -u)
            echo $FILE_TYPES

            # Check if the only file type is markdown
            if [[ "$FILE_TYPES" == "md" ]]; then
              echo "All files are markdown, skipping step."
              echo "SKIP_TESTS=true" >> $GITHUB_ENV
            fi

        # Checkout master branch of gen3-code-vigil when another repo is under test
        - name: Checkout integration test code
          if: ${{ env.SKIP_TESTS != 'true' && github.event.repository.name  != 'gen3-code-vigil' }}
          uses: actions/checkout@v4
          with:
            repository: uc-cdis/gen3-code-vigil
            ref: ${{ inputs.TEST_REPO_BRANCH }}

        # gen3-integration-tests run with python 3.9
        - name: Set up Python
          if: ${{ env.SKIP_TESTS != 'true' }}
          uses: actions/setup-python@v5
          with:
            python-version: '3.9'

        - name: Set up Go
          if: ${{ env.SKIP_TESTS != 'true' }}
          uses: actions/setup-go@v5
          with:
            go-version: '1.17'

        # allure report generation needs node
        - name: Set up node
          if: ${{ env.SKIP_TESTS != 'true' }}
          uses: actions/setup-node@v4
          with:
            node-version: 20

        # Setup kubectl
        - name: Set Up kubectl
          uses: azure/setup-kubectl@v3
          with:
            version: latest

        # # Install gen3-integration-tests dependencies
        # # wamerican: data-simulator needs "/usr/share/dict/words" to generate data that isn't random strings
        # - name: Install dependencies
        #   if: ${{ env.SKIP_TESTS != 'true' }}
        #   run: |
        #     sudo apt-get install -y --reinstall wamerican
        #     python -m pip install --upgrade pip
        #     pip install poetry
        #     poetry install
        #     poetry show
        #     poetry run playwright install chromium

        # Configure credentials to access cluster.
        - name: Configure AWS credentials
          if: ${{ env.SKIP_TESTS != 'true' }}
          uses: aws-actions/configure-aws-credentials@v1
          with:
            aws-access-key-id: ${{ secrets.CI_AWS_ACCESS_KEY_ID }}
            aws-secret-access-key: ${{ secrets.CI_AWS_SECRET_ACCESS_KEY }}
            aws-region: "us-east-1"

        # Create kubeconfig to point at cluster.
        - name: Update kube config
          if: ${{ env.SKIP_TESTS != 'true' }}
          run: |
            aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region "us-east-1"
            aws sts get-caller-identity

        # NOTE: Add in repo name
        # Create PR namespace and PR hostname env var
        - name: Create namespace and env vars
          if: ${{ env.SKIP_TESTS != 'true' }}
          run: |
            PR_NAMESPACE=pr-${PR_NUM}
            PR_HOSTNAME=pr${PR_NUM}
            echo "PR_NAMESPACE=$PR_NAMESPACE" >> $GITHUB_ENV
            echo "PR_HOSTNAME=$PR_HOSTNAME" >> $GITHUB_ENV
            echo "ARTIFACT_PATH=$REPO/$PR_NUM/$RUN_NUM" >> $GITHUB_ENV
            kubectl create namespace $PR_NAMESPACE || true

        # Setup helm
        - name: Set Up Helm
          if: ${{ env.SKIP_TESTS != 'true' }}
          uses: azure/setup-helm@v4.3.0
          with:
            version: latest

        # This is used for running specific test suites by labeling the PR with the test class
        # Multiple suites can be executed by adding multiple labels
        - name: Get test labels
          id: get_test_labels
          if: ${{ env.SKIP_TESTS != 'true' }}
          continue-on-error: true  # if this fails, we still need to run clean-up steps
          run: |
            test_label=$(gh api repos/$REPO_FN/pulls/$PR_NUM --jq '.labels | map(select(.name | startswith("Test"))) | map(.name) | if length > 0 then "-k \"" + join(" or ") + "\"" else "" end')
            echo $test_label
            echo "TEST_LABEL=$test_label" >> $GITHUB_ENV

        # Will install the gen3 helm charts to specific PR namespace and set the tests to run
        - name: Prepare CI environment
          id: prep_ci_env
          if: ${{ env.SKIP_TESTS != 'true' }}
          continue-on-error: true  # if this fails, we still need to run clean-up steps
          run: |
            # helm repo add gen3 https://helm.gen3.org
            # helm repo update
            # uncomment the above after testing ^^^^

            # TO DO: add all of this to new modify values script, including new ES index dbRestore:
            # disabling guppy until the above is completed.
            # remove usersync run from helm test job as well.

            git clone https://github.com/uc-cdis/gen3-helm.git
            cd gen3-helm/helm/gen3
            git checkout feat/ci-patch
            # https://github.com/uc-cdis/cloud-automation/blob/master/.github/workflows/integration_tests.yaml#L15
            helm dependency update
            cd ..
            pwd
            helm upgrade --install gen3 gen3 --set tests.TEST_LABEL="${{ env.TEST_LABEL }}" --set tests.artifactCreds.secondaryFullPath="${{ secrets.QA_DASHBOARD_S3_PATH }}/$REPO/$PR_NUM/$RUN_NUM" --set tests.artifactCreds.artifactPath="${{ env.ARTIFACT_PATH }}" --set tests.SERVICE_TO_TEST="${{ inputs.SERVICE_TO_TEST }}" --set tests.artifactCreds.artifactBucketName="${{ secrets.QA_HELM_ARTIFACT_S3_PATH }}" --set tests.artifactCreds.awsAccessKeyId="${{ secrets.CI_AWS_ACCESS_KEY_ID }}"  --set tests.artifactCreds.awsSecretAccessKey="${{ secrets.CI_AWS_SECRET_ACCESS_KEY }}" --set global.hostname="${{ env.PR_HOSTNAME }}.ci.planx-pla.net" -f /runner/_work/gen3-code-vigil/gen3-code-vigil/gen3-integration-tests/helm_values/values.yaml \
              --namespace ${{ env.PR_NAMESPACE }}

            # helm upgrade --install gen3 gen3/gen3 --set tests.TEST_LABEL="${{ env.TEST_LABEL }}" --set tests.SERVICE_TO_TEST="${{ inputs.SERVICE_TO_TEST }}" --set global.hostname="${{ env.PR_HOSTNAME }}.ci.planx-pla.net" -f /runner/_work/gen3-code-vigil/gen3-code-vigil/gen3-integration-tests/helm_values/values.yaml \
            #   --namespace ${{ env.PR_NAMESPACE }}
            # uncomment after testing
            sleep 60

            export timeout=900
            export interval=20

            end=$((SECONDS + timeout))
            while [ $SECONDS -lt $end ]; do
              # Get JSON for not-ready, non-terminating pods
              not_ready_json=$(kubectl get pods -l app!=gen3job -n "${{ env.PR_NAMESPACE }}" -o json | \
                jq '[.items[]
                  | select(
                      (.metadata.deletionTimestamp == null) and
                      ((.status.phase != "Running") or
                      (.status.containerStatuses[]?.ready != true))
                    )
                ]')

              not_ready_count=$(echo "$not_ready_json" | jq 'length')

              if [ "$not_ready_count" -eq 0 ]; then
                echo "âœ… All pods containers are Ready"
                exit 0
              fi

              echo "â³ Waiting... ($not_ready_count pods have containers not ready)"
              sleep $interval
            done

            echo "âŒ Timeout: Pods' containers not ready"
            echo "$not_ready_json" | jq -r '.[] |
              .metadata.name as $pod_name |
              .status.containerStatuses[]?
              | select(.ready != true)
              | "ðŸ”´ Pod: \($pod_name), Container: \(.name) is NOT ready"'

            kubectl get pods -n "${{ env.PR_NAMESPACE }}"
            exit 1

        - name: Run tests pertaining to specific service
          id: run_service_tests
          if: ${{ env.SKIP_TESTS != 'true' && inputs.SERVICE_TO_TEST && steps.prep_ci_env.outcome == 'success' }}
          continue-on-error: true  # if this fails, we still need to run clean-up steps
          run: |
            mkdir output
            GEN3_INSTANCE_TYPE="ADMINVM_REMOTE" poetry run pytest -n auto -m "${{ inputs.SERVICE_TO_TEST }} and not wip" --alluredir allure-results --no-header --dist loadscope ${{ env.TEST_LABEL }}
            if [ $? -ne 0 ]; then
              echo "PR_ERROR_MSG=Test(s) failures encountered in PR" >> $GITHUB_ENV
            fi

        - name: Run tests
          id: run_tests
          if: ${{ env.SKIP_TESTS != 'true' && !inputs.SERVICE_TO_TEST && steps.prep_ci_env.outcome == 'success' }}
          continue-on-error: true  # if this fails, we still need to run clean-up steps
          run: |
            mkdir output
            GEN3_INSTANCE_TYPE="ADMINVM_REMOTE" poetry run pytest -n auto -m "not wip" --alluredir allure-results --no-header --dist loadscope ${{ env.TEST_LABEL }}
            if [ $? -ne 0 ]; then
              echo "PR_ERROR_MSG=Test(s) failures encountered in PR" >> $GITHUB_ENV
            fi

        - name: Debug logging
          if: ${{ env.SKIP_TESTS != 'true' }}
          continue-on-error: true  # if this fails, we still need to run clean-up steps
          run: |
            echo steps.run_tests.outcome = ${{ steps.run_tests.outcome }}

        - name: Generate allure report
          id: generate_allure_report
          if: ${{ env.SKIP_TESTS != 'true' && steps.run_service_tests.outcome == 'success' || steps.run_service_tests.outcome == 'failure' || steps.run_tests.outcome == 'success' || steps.run_tests.outcome == 'failure' }}
          continue-on-error: true  # if this fails, we still need to run clean-up steps
          run: |
            npm install -g allure-commandline --save-dev
            allure generate allure-results -o allure-report --clean
            if [ $? -ne 0 ]; then
              echo "PR_ERROR_MSG=Failed to generate allure report" >> $GITHUB_ENV
            fi

        - name: Upload allure report to S3
          id: upload_allure_report
          if: ${{ env.SKIP_TESTS != 'true' && steps.generate_allure_report.outcome == 'success' }}
          continue-on-error: true  # if this fails, we still need to run clean-up steps
          run: |
            aws s3 sync ./allure-report ${{ secrets.QA_DASHBOARD_S3_PATH }}/$REPO/$PR_NUM/$RUN_NUM
            if [ $? -ne 0 ]; then
              echo "PR_ERROR_MSG=Failed to upload allure report to s3 bucket" >> $GITHUB_ENV
            fi
          env:
            AWS_ACCESS_KEY_ID: ${{ secrets.CI_AWS_ACCESS_KEY_ID }}
            AWS_SECRET_ACCESS_KEY: ${{ secrets.CI_AWS_SECRET_ACCESS_KEY }}
            AWS_DEFAULT_REGION: 'us-east-1'

        - name: Generate markdown report
          id: generate_md_report
          if: ${{ env.SKIP_TESTS != 'true' && steps.download_allure_summary.outcome == 'success' }}
          continue-on-error: true
          # It is possible for env is prepped but tests error out, the pod logs will help in debugging env issues
          run: |
            if [ -n "${{ env.PR_ERROR_MSG }}" ]; then
              echo -e "\n${{ env.PR_ERROR_MSG }}" >> output/report.md
            fi
            if [ "${{ steps.download_allure_summary.outcome }}" == "success" ]; then
              echo -e "\nPlease find the detailed integration test report [here](https://qa.planx-pla.net/dashboard/Secure/gen3-ci-reports/$REPO/$PR_NUM/$RUN_NUM/index.html)\n" >> output/report.md
            fi
            if [ "${{steps.archive_pod_logs.outcome}}" == "success" ]; then
              if [ ! -d output ]; then
                mkdir output
              fi
              if [ ! -f "output/report.md" ]; then
                touch "output/report.md"
              fi
              echo -e "Please find the ci env pod logs [here]($POD_LOGS_URL)\n" >> output/report.md
            fi

        - name: Render report to the PR
          id: publish_md_report
          if: ${{ env.SKIP_TESTS != 'true' && steps.download_allure_summary.outcome == 'success' }}
          continue-on-error: true  # if this fails, we still need to run clean-up steps
          run: gh pr comment $PR_NUM --body-file output/report.md -R $REPO_FN
          env:
            GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

        - name: Generate Slack report
          id: generate_slack_report
          if: ${{ env.SKIP_TESTS != 'true' && steps.generate_allure_report.outcome == 'success' || steps.archive_pod_logs.outcome == 'success' }}
          continue-on-error: true # if this fails, we still need to run clean-up steps
          run: poetry run python -m gen3_ci.scripts.generate_slack_report

        - name: Publish report to Slack
          id: slack_notify
          # if: ${{ env.SKIP_TESTS != 'true' && steps.publish_md_report.outcome == 'success' }}
          # continue-on-error: true  # if this fails, we still need to run clean-up steps
          uses: slackapi/slack-github-action@v2.0.0
          with:
            method: chat.postMessage
            token: ${{ secrets.CI_SLACK_BOT_TOKEN }}
            payload-file-path: "./gen3-integration-tests/slack_report.json"
            payload-templated: true

        - name: Mark workflow as failed for unsuccessful test runs
          if: ${{ env.SKIP_TESTS != 'true' && steps.run_tests.outcome != 'success' }}
          run: echo "Test run was unsuccessful, marking workflow as failed" && exit 1

        - name: Cleanup
          if: ${{ env.SKIP_TESTS != 'true' }}
          run: |
            # helm delete gen3 --namespace ${{ env.PR_NAMESPACE }}
            kubectl delete jobs -l "helm.sh/hook=test" -n ${{ env.PR_NAMESPACE }}
            # sleep 20
            # kubectl delete ns ${{ env.PR_NAMESPACE }}
